{"cells":[{"cell_type":"markdown","metadata":{"id":"JKdfRsPZOx7-"},"source":["This exercise represents a complete machine learning process from preprocessing to model performance testing by using the functions developed using only NumPy without using any other libraries like pandas, scikit-learn, or scipy.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9BPzS0g9Ox7-"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math"]},{"cell_type":"markdown","metadata":{"id":"svj3G7hj4ZxY"},"source":["Create a model and even test it - by dividing the dataset into training and test sets. \n","To practice NumPy efficiently. I have used only the values of the dataset, ignoring Pandas properties such as column names.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2HAUc00Wmu0E"},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","\n","california_housing = fetch_california_housing(as_frame=False)\n","\n","X = california_housing.data\n","y = california_housing.target"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"u4KbMNxMm9Oi"},"outputs":[{"data":{"text/plain":["array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n","          37.88      , -122.23      ],\n","       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n","          37.86      , -122.22      ],\n","       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n","          37.85      , -122.24      ],\n","       ...,\n","       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n","          39.43      , -121.22      ],\n","       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n","          39.43      , -121.32      ],\n","       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n","          39.37      , -121.24      ]])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["#X.describe()\n","X"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9rTO_NYMNHlg"},"outputs":[{"data":{"text/plain":["array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"markdown","metadata":{"id":"Fbj_IpkHOx8A"},"source":["### 1. Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"EE0ALwTyOx8A"},"source":["The first task is to open the dataset and preprocess it into the form that the model can understand. It involves imputation, train_test_split, standardization, and normalization. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"t2uFZSUuoh9t"},"source":["First,I have developed both standardization and normalization functions.\n","\n","\n","- Standardization: Make features have the same standard deviaton and mean.\n","\n","- Normalization: Make the range of value normalized into [0, 1]. This means that each column's minimum value should be zero and maximum value should be one."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bQRVVievOx8A"},"outputs":[],"source":["def standardize(data):\n","\n","  #Input: NumPy ndarray\n","  #Output: NumPy ndarray with column mean == 0 and std == 1\n","  \n","  data_standardize = np.zeros(data.shape)\n","  \n","  for i in range(data.shape[1]):\n","    \n","      np_mean = np.mean(data[:, i])    \n","      np_std = np.std(data[:, i])\n","      data_standardize[:,i] = (data[:, i] - np_mean) /  np_std\n","  \n","    \n","  \n","  return data_standardize\n","\n","def normalize(data):\n","  \"\"\"\"\n","  Input: NumPy ndarray\n","  Output: NumPy ndarray with column min == 0 and max == 1\n","  \"\"\"\n","  data_normalize = np.zeros(data.shape)\n","  \n","  for i in range(data.shape[1]):\n","    \n","    min_value = np.min(data[:, i])\n","    max_value = np.max(data[:, i])\n","  \n","    data_normalize[:, i] = (data[:, i] - min_value) / (max_value - min_value)\n","  \n","  return data_normalize\n"]},{"cell_type":"markdown","metadata":{"id":"Cb4WgGQ0o_9G"},"source":["Let's apply both functions separately and create X_standardized and X_normalized."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2qf7fOZG54MA"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 2.34476576  0.98214266  0.62855945 ... -0.04959654  1.05254828\n","  -1.32783522]\n"," [ 2.33223796 -0.60701891  0.32704136 ... -0.09251223  1.04318455\n","  -1.32284391]\n"," [ 1.7826994   1.85618152  1.15562047 ... -0.02584253  1.03850269\n","  -1.33282653]\n"," ...\n"," [-1.14259331 -0.92485123 -0.09031802 ... -0.0717345   1.77823747\n","  -0.8237132 ]\n"," [-1.05458292 -0.84539315 -0.04021111 ... -0.09122515  1.77823747\n","  -0.87362627]\n"," [-0.78012947 -1.00430931 -0.07044252 ... -0.04368215  1.75014627\n","  -0.83369581]]\n","[[0.53966842 0.78431373 0.0435123  ... 0.00149943 0.5674814  0.21115538]\n"," [0.53802706 0.39215686 0.03822395 ... 0.00114074 0.565356   0.21215139]\n"," [0.46602805 1.         0.05275646 ... 0.00169796 0.5642933  0.21015936]\n"," ...\n"," [0.08276438 0.31372549 0.03090386 ... 0.0013144  0.73219979 0.31175299]\n"," [0.09429525 0.33333333 0.03178269 ... 0.0011515  0.73219979 0.30179283]\n"," [0.13025338 0.29411765 0.03125246 ... 0.00154886 0.72582359 0.30976096]]\n"]}],"source":["X_standardized = standardize(X)\n","X_normalized = normalize(X)\n","\n","print(X_standardized)\n","print(X_normalized)"]},{"cell_type":"markdown","metadata":{"id":"So46vKNl53KL"},"source":["To check if the functions are correctly made. Created a function to check the dataset's min, max, mean, std of each feature. "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"l9BqVsG36Tsl"},"outputs":[],"source":["def describe(data):\n","  \n","  data_min = np.zeros(data.shape[1])\n","  data_max = np.zeros(data.shape[1])\n","  data_mean = np.zeros(data.shape[1])\n","  data_std = np.zeros(data.shape[1])\n","  \n","  for i in range(data.shape[1]):\n","    \n","    data_min[i] = np.min(data[:, i])  \n","    data_max[i] = np.max(data[:, i])\n","    data_mean[i] = np.mean(data[:, i])\n","    data_std[i] = np.std(data[:, i])\n","  \n","  print(\"Min: \", data_min)\n","  print(\"Max: \", data_max)\n","  print(\"Mean: \", data_mean)\n","  print(\"Std: \", data_std)\n","  \n","  return None"]},{"cell_type":"markdown","metadata":{"id":"FZEPPjpJ6W3B"},"source":["Using this function, let's check if **standardize** and **normalize** functions are correctly working.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1674746704414,"user":{"displayName":"Zed Lee","userId":"17112477486863490687"},"user_tz":-60},"id":"1NY6wtki6c84","outputId":"945364b8-7158-4052-a701-a279cfef5ea1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [-1.77429947 -2.19618048 -1.8523186  -1.61076772 -1.25612255 -0.22899997\n"," -1.447568   -2.38599234]\n","Max:  [  5.85828581   1.85618152  55.16323628  69.57171326  30.25033022\n"," 119.41910319   2.95806762   2.62528006]\n","Mean:  [ 6.60969987e-17  5.50808322e-18  6.60969987e-17 -1.06030602e-16\n"," -1.10161664e-17  3.44255201e-18 -1.07958431e-15 -8.52651283e-15]\n","Std:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"]}],"source":["describe(X_standardized)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1674746704708,"user":{"displayName":"Zed Lee","userId":"17112477486863490687"},"user_tz":-60},"id":"G7-p5vdl3H4F","outputId":"b179dea5-39dd-4eab-8b1e-ade4f6160794"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [0. 0. 0. 0. 0. 0. 0. 0.]\n","Max:  [1. 1. 1. 1. 1. 1. 1. 1.]\n","Mean:  [0.23246376 0.54195071 0.03248795 0.02262871 0.03986874 0.00191395\n"," 0.32857188 0.47612505]\n","Std:  [0.13101721 0.24676966 0.01753907 0.0140484  0.03173953 0.00835784\n"," 0.226982   0.19955012]\n"]}],"source":["describe(X_normalized)"]},{"cell_type":"markdown","metadata":{"id":"mj47zZ264Az0"},"source":["However, this is not a complete setting, we need to both train the model and test it. That means we need to divide the dataset into two parts: {a training set, a test set} and only use the training set to train the model. This means that we also need to create the function for it."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ORFET61FbZVy"},"outputs":[],"source":["def train_test_split(X, y, test_ratio = 0.3):\n","  # simulation\n","  # cross-val\n","\n","  \"\"\"\n","  Input:\n","    - X: a set of features\n","    - y: corresponding labels\n","    - test_ratio: ratio of the test set\n","\n","  Output:\n","    - X_train: separated training instances\n","    - X_test: separated test instances\n","    - y_train: separated training labels\n","    - y_test: separated test labels\n","\n","\n","  \"\"\"\n","  length = len(X)\n","  \n","  indices = np.random.permutation(length)     # Randomly shuffle the indices of the data instances\n","  n_test = int(length * test_ratio)\n","  \n","  test_indices = indices[:n_test]             # Divide the indices into two parts with the ratio of [1-test ratio:test ratio]\n","  train_indices = indices[n_test:]\n","  \n","  X_train = X[train_indices]                  #  Select training instances and labels with the first set of indices and test instances and labels with the second set of indices\n","  y_train = y[train_indices]\n","  \n","  X_test = X[test_indices]\n","  y_test = y[test_indices]\n","  \n","  return X_train, X_test, y_train, y_test    #  Return the training set and the test set"]},{"cell_type":"markdown","metadata":{"id":"twOo6_yniUGO"},"source":["Split the dataset into training and test sets with `test ratio = 0.33`."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"f-iSb7IoijO1"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X,y,0.3)"]},{"cell_type":"markdown","metadata":{"id":"d9PK6RGbkK7O"},"source":["After applying train_test_split function, we can check the shape of each subset. The training set should have 14,448 rows while the test set might have 6,192 records. "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"bk_pMMo-iquK"},"outputs":[{"data":{"text/plain":["((14448, 8), (6192, 8), (14448,), (6192,))"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{"id":"7T8JBhsTj00O"},"source":["\n","\n","- Create two functions (**apply_standardization**, **apply_normalization**) that uses training set's statistics and apply standardization or normalization to both sets."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XLs3P181kYHA"},"outputs":[],"source":["def apply_standardization(X_train, X_test):\n","  \"\"\"\n","  Input:\n","    - X_train: training instances\n","    - X_test: test instances\n","\n","  Output:\n","    - X_train_standardized\n","    - X_test_standardized\n","\n","  \"\"\"\n","  X_train_standardized = np.zeros(X_train.shape)\n","  X_test_standardized = np.zeros(X_test.shape)\n","  \n","  for i in range(X_train.shape[1]):\n","    \n","    train_mean = np.mean(X_train[:, i])\n","    train_std = np.std(X_train[:,i])\n","   \n","    X_train_standardized[:,i] = (X_train[:,i] - train_mean) / train_std\n","    X_test_standardized[:,i] = (X_test[:,i] - train_mean ) / train_std\n","      \n","  return X_train_standardized, X_test_standardized "]},{"cell_type":"code","execution_count":14,"metadata":{"id":"sOx7Kd221jRq"},"outputs":[],"source":["def apply_normalization(X_train, X_test):\n","  \"\"\"\n","  Input:\n","    - X_train\n","    - X_test\n","\n","  Output:\n","    - X_train_standardized\n","    - X_test_standardized\n","  \"\"\"\n","  X_train_normalized = np.zeros(X_train.shape)\n","  X_test_normalized = np.zeros(X_test.shape)\n","  \n","  for i in range(X_train.shape[1]):\n","    \n","    train_min = np.min(X_train[:, i])\n","    train_max = np.max(X_train[:,i])\n","   \n","    X_train_normalized[:,i] = (X_train[:,i] - train_min) / (train_max - train_min)\n","    X_test_normalized[:,i] = (X_test[:,i] - train_min) / (train_max - train_min)\n","      \n","  return X_train_normalized, X_test_normalized\n"]},{"cell_type":"markdown","metadata":{"id":"ujWQ84ZFCa10"},"source":["- Apply two functions (**apply_standardization**, **apply_normalization**) to created standardized and normalized datasets."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Cw8tjyoCtnqI"},"outputs":[],"source":["X_train_standardized, X_test_standardized = apply_standardization(X_train, X_test)\n","X_train_normalized, X_test_normalized = apply_normalization(X_train, X_test)"]},{"cell_type":"markdown","metadata":{"id":"efxhH-DvCxBx"},"source":["Check the statistics using describe method. Test set should **NOT** have zero mean and standard deviation 1 or zero min and one max. Good test set however might show close value to zero or one."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ch5zQTDDDugQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [-1.77706013 -2.20031331 -1.74408475 -1.51546111 -1.25468609 -0.19610802\n"," -1.44080675 -2.39405564]\n","Max:  [  5.85945965   1.86374841  52.30737797  65.26324504  30.27741757\n"," 100.29204103   2.96736121   2.62385691]\n","Mean:  [-2.96059473e-16  4.69662453e-17 -1.71389911e-16 -6.68838677e-17\n"," -9.44242838e-17 -1.77045532e-17  1.03915892e-15  4.63121605e-15]\n","Std:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"]}],"source":["describe(X_train_standardized)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"8qyxhAClDx7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [-1.77706013 -2.20031331 -1.76046461 -1.12366411 -1.25026723 -0.16400576\n"," -1.44549629 -2.36906603]\n","Max:  [ 5.85945965  1.86374841 21.84095219 25.75910926 23.98849552  6.47369767\n","  2.93453442  2.54389018]\n","Mean:  [-0.00615343  0.00735947 -0.01303715 -0.01448379  0.00819708 -0.01259721\n","  0.01481537 -0.01633918]\n","Std:  [1.00170403 1.00957289 0.81493748 0.77470718 1.0026878  0.13339687\n"," 1.00537732 1.00431698]\n"]}],"source":["describe(X_test_standardized)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"_tTsEozJ2_3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [0. 0. 0. 0. 0. 0. 0. 0.]\n","Max:  [1. 1. 1. 1. 1. 1. 1. 1.]\n","Mean:  [0.2327055  0.54140745 0.03226711 0.02269378 0.03979075 0.00195155\n"," 0.32684933 0.47710191]\n","Std:  [0.1309497  0.24605926 0.01850089 0.01497483 0.03171371 0.00995142\n"," 0.22685161 0.19928606]\n"]}],"source":["describe(X_train_normalized)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"bNplCpND3Byu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [ 0.          0.         -0.00030304  0.00586709  0.00014014  0.00031946\n"," -0.00106383  0.00498008]\n","Max:  [1.         1.         0.4363441  0.40843215 0.80055495 0.06637405\n"," 0.99255319 0.98406375]\n","Mean:  [0.23189971 0.54321832 0.03202592 0.02247689 0.04005071 0.00182619\n"," 0.33021022 0.47384573]\n","Std:  [0.13117285 0.24841475 0.01507707 0.01160111 0.03179895 0.00132749\n"," 0.22807146 0.20014637]\n"]}],"source":["describe(X_test_normalized)"]},{"cell_type":"markdown","metadata":{"id":"EgG7BAsDOx8B"},"source":["### 2. Linear regression"]},{"cell_type":"markdown","metadata":{"id":"aDLWBj56D6P6"},"source":["Now the dataset is ready to train a model. We will use the linear regression.\n","\n","- Created the **solver** function that creates a linear regression line and return the coefficents. \n","- Here we have used **all available features** of the dataset.\n","- Added one column representing a bias to your feature matrix.\n","\n","The normal equation can be represented as follows:\n","\n","$\\theta = (\\textbf{X}^T \\cdot \\textbf{X})^{-1} \\cdot \\textbf{X}^T \\cdot \\textbf{y}$"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"6EkjV6APOx8C"},"outputs":[],"source":["def solver(X, y):\n","  \"\"\"\n","  Get the weights and bias of linear regression classifier on the input dataset (X, y).\n","\n","  Input:\n","   - X: a set of features\n","   - y: labels\n","  Output:\n","   - theta: weights and bias of the linear regression\n","  \"\"\"\n","  array_ones = np.ones((X.shape[0],1))\n","  X_bias = np.append( X,array_ones, axis = 1)\n","  #print(X_add)\n","  \n","  X_T = np.transpose(X_bias)\n","  theta = (np.linalg.inv((X_T.dot(X_bias)))).dot(X_T).dot(y)\n","  \n","  return theta.tolist()"]},{"cell_type":"markdown","metadata":{"id":"xWZhZW4rExeQ"},"source":["Run this solver function only on the standardized training set (**X_train_standardized**, **y_train**) to create the model and evalute it later on the test set.\n","\n","- Run the **solve** function on **X_train_standardized** and **y_train** and save the result to **theta**."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1642967292623,"user":{"displayName":"Zed Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17112477486863490687"},"user_tz":-60},"id":"4_8ETLDgE_5p","outputId":"390908bc-5d45-4e55-f9c3-5b043bc2f0db"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.8202734389355764, 0.12044450056737013, -0.26226512727843854, 0.3064312382619917, -0.0033497792154896415, -0.040030071379064565, -0.8910747059297772, -0.8603840854496043, 2.069132241832785]\n"]}],"source":["theta = solver(X_train_standardized, y_train)\n","print(theta)"]},{"cell_type":"markdown","metadata":{"id":"dQDfYDDxF9o5"},"source":["Now we have a complete model trained on the training set. Then the next interesting thing is to evaluate if the model is good enough by using the test set. To do this, we need to create a predict function that can return the expected value.\n","\n","- Create the **predict** function which put each instance into the regression equation to predict the value."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"p-JxOI09ledN"},"outputs":[],"source":["def predict(X, theta):\n","  \"\"\"\n","  Input:\n","   - X: data instances to predict\n","   - theta: trained regression coefficients\n","\n","  Output:\n","   - y_hat: predicted values (X @ weight) + bias\n","  \"\"\"\n","  bias = theta[0]\n","  weight = theta[1:]\n","  y_hat = X.dot(weight) + bias\n","  return y_hat\n","\n","#theta = solver(X_train_standardized, y_train)\n","#print(predict(X_train_standardized,theta))"]},{"cell_type":"markdown","metadata":{"id":"psp-L1wGRslu"},"source":["This predict function should be able to return the predicted value of the housing price. Now we might want to return the mean squared error (and its variants) of the whole model. There can be many different metrics but here we will measure the rooted mean squared error (RMSE). RMSE can be calculated as follows:"]},{"cell_type":"markdown","metadata":{"id":"ljB3uUqjWPbw"},"source":["$RMSE = \\sqrt{\\frac{1}{n}\\sum_{t=1}^{n}(\\hat{y}_t - y_t)^2} $.\n","\n","Note that $\\hat{y}$ is a predicted label and $y$ is a true label."]},{"cell_type":"markdown","metadata":{"id":"aMTnVQGPWfBv"},"source":["- Created a function **rooted_mean_squared_error** that calculates the RMSE value."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"laAN9bTMOx8C"},"outputs":[],"source":["def rooted_mean_squared_error(X, y, theta):\n","  \"\"\"\n","  Input:\n","    - X_test: data instances to test\n","    - y_test: true class labels of the corresponding data instances (X_test)\n","    - theta: trained regression coefficients\n","\n","  Output:\n","    - RMSE: the RMSE score\n","\n","  Use predict function to calculate our predicted values.\n","  \"\"\"\n","  y_hat = predict(X, theta)\n","  n = len(X)\n","  sum = 0\n","  for i in range(n):\n","    y_square = (y_hat[i] - y[i])**2\n","    sum = sum + y_square\n","  \n","  rmse_square = sum/n \n","  rmse = math.sqrt(rmse_square)\n","  return rmse"]},{"cell_type":"markdown","metadata":{"id":"52aH-ML6Seu7"},"source":["Even though the RMSE is generally the preferred performance measure for\n","regression tasks, in some contexts we may prefer to use another function. For\n","example, suppose that there are many outliers. In that case, we may\n","consider using the mean absolute error (MAE). It's direct translation of l1 and l2 norm. The higher the norm index, the more it focuses on large values and\n","neglects small ones. This is why the RMSE is more sensitive to\n","outliers than the MAE. But when outliers are exponentially rare (like\n","in a bell-shaped curve), the RMSE performs very well and is\n","generally preferred.\n","\n","MAE can be calculated as follows:\n","\n","$MAE = \\frac{1}{n}\\sum_{t=1}^{n}|\\hat{y}_t - y_t|$\n","\n","- Implemented a function for MAE **mean_absolute_error**, which receives the same parameters *X*, *y*, and *theta*."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"uFEGAF7fqqeg"},"outputs":[],"source":["def mean_absolute_error(X, y, theta):\n","  \"\"\"\n","  Input:\n","    - X_test: data instances to test\n","    - y_test: true values of the corresponding data instances (X_test)\n","    - theta: trained regression coefficients\n","\n","  Output:\n","    - MAE: MAE score\n","\n","  Use predict function to calculate our predicted values.\n","  \"\"\"\n","  y_hat = predict(X,theta)\n","  n= len(X)\n","  sum = 0\n","  for i in range(n):\n","    sum = sum + (abs(y_hat[i] - y[i]))\n","\n","  mae_value = sum / n\n","              \n","  return mae_value"]},{"cell_type":"markdown","metadata":{"id":"PNC0hszHqyIP"},"source":["Train the regression model on the **standardized** training set and evaluate the method with two different scores: RMSE and MAE. "]},{"cell_type":"code","execution_count":25,"metadata":{"id":"wm8gHjCSq64P"},"outputs":[],"source":["rmse_score = rooted_mean_squared_error(X_test_standardized, y_test, theta)\n","mae_score = mean_absolute_error(X_test_standardized, y_test, theta)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1642967292624,"user":{"displayName":"Zed Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17112477486863490687"},"user_tz":-60},"id":"LWaJtiItrmDk","outputId":"ba786a85-0aad-45b1-9c6e-57b1a7be2cba"},"outputs":[{"data":{"text/plain":["(3.3613677147955863, 2.681828339015123)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["rmse_score, mae_score"]},{"cell_type":"markdown","metadata":{"id":"royxfFHjRjw2"},"source":["### 4. Linear regression with regularization"]},{"cell_type":"markdown","metadata":{"id":"wphcDNvvkwL0"},"source":["Fortunately, the Ridge regression also can be represented as a closed form solution with the normal equation.\n","\n","Here Created a variant of previous solver function supporting the Ridge regression.\n","\n","A closed form solution to Ridge can be represented as follows:\n","\n","$\\theta = (\\textbf{X}^T \\cdot \\textbf{X} + \\lambda \\textbf{I})^{-1} \\cdot \\textbf{X}^T \\cdot \\textbf{y}$\n","\n","where $\\textbf{I}$ is an $(n+1) \\times (n+1) $ identity matrix, since the feature matrix also includes the bias column.\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jvYlCgotRlcK"},"outputs":[],"source":["def solver_with_ridge(X, y, alpha):\n","  \n"," \n","  array_ones = np.ones((X.shape[0],1))\n","  \n","  X_bias = np.append( X , array_ones, axis = 1)\n","  Iden_matrx = np.identity(X_bias.shape[1])\n","  X_T = np.transpose(X_bias)\n"," \n","  theta = (np.linalg.inv(X_T.dot(X_bias) + (Iden_matrx * (alpha)))).dot(X_T).dot(y)\n","\n","  return theta.tolist()"]},{"cell_type":"markdown","metadata":{"id":"G-LrDqWSmaGW"},"source":["Here, comparing the performances changing the $\\lambda$ value. Use the $\\lambda$ value from 0 to 30 in increments of 0.2. Used RMSE as a score metric. Then saved those 300 scores into the list `scores`.\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Vd9pJ8zUo_7u"},"outputs":[{"data":{"text/plain":["[3.3613677147955863,\n"," 3.361200781906101,\n"," 3.361033918403565,\n"," 3.3608671242387294,\n"," 3.3607003993624236,\n"," 3.3605337437254983,\n"," 3.3603671572788656,\n"," 3.3602006399735065,\n"," 3.36003419176043,\n"," 3.359867812590689,\n"," 3.3597015024154118,\n"," 3.359535261185747,\n"," 3.359369088852923,\n"," 3.3592029853681855,\n"," 3.359036950682868,\n"," 3.358870984748311,\n"," 3.3587050875159363,\n"," 3.358539258937201,\n"," 3.358373498963615,\n"," 3.3582078075467376,\n"," 3.3580421846381654,\n"," 3.3578766301895695,\n"," 3.3577111441526504,\n"," 3.3575457264791626,\n"," 3.3573803771209048,\n"," 3.3572150960297233,\n"," 3.357049883157533,\n"," 3.3568847384562677,\n"," 3.356719661877948,\n"," 3.3565546533745954,\n"," 3.3563897128983036,\n"," 3.3562248404012305,\n"," 3.3560600358355632,\n"," 3.355895299153535,\n"," 3.3557306303074332,\n"," 3.3555660292495952,\n"," 3.3554014959324014,\n"," 3.35523703030828,\n"," 3.35507263232973,\n"," 3.354908301949253,\n"," 3.3547440391194314,\n"," 3.354579843792888,\n"," 3.354415715922296,\n"," 3.3542516554603745,\n"," 3.3540876623598774,\n"," 3.3539237365736314,\n"," 3.353759878054475,\n"," 3.3535960867553354,\n"," 3.3534323626291425,\n"," 3.353268705628927,\n"," 3.353105115707723,\n"," 3.3529415928186195,\n"," 3.3527781369147722,\n"," 3.3526147479493518,\n"," 3.3524514258756204,\n"," 3.3522881706468377,\n"," 3.3521249822163366,\n"," 3.3519618605375148,\n"," 3.3517988055637615,\n"," 3.351635817248562,\n"," 3.3514728955454425,\n"," 3.351310040407951,\n"," 3.351147251789698,\n"," 3.350984529644339,\n"," 3.3508218739255846,\n"," 3.3506592845871617,\n"," 3.3504967615828822,\n"," 3.3503343048665757,\n"," 3.350171914392131,\n"," 3.350009590113475,\n"," 3.3498473319846007,\n"," 3.349685139959509,\n"," 3.3495230139922767,\n"," 3.3493609540370155,\n"," 3.349198960047898,\n"," 3.3490370319791154,\n"," 3.3488751697849297,\n"," 3.348713373419614,\n"," 3.3485516428375464,\n"," 3.348389977993092,\n"," 3.348228378840675,\n"," 3.348066845334783,\n"," 3.3479053774299454,\n"," 3.347743975080725,\n"," 3.3475826382417253,\n"," 3.3474213668676187,\n"," 3.347260160913096,\n"," 3.347099020332906,\n"," 3.346937945081842,\n"," 3.346776935114744,\n"," 3.3466159903864847,\n"," 3.346455110851999,\n"," 3.346294296466256,\n"," 3.346133547184272,\n"," 3.345972862961108,\n"," 3.3458122437518507,\n"," 3.3456516895116692,\n"," 3.3454912001957493,\n"," 3.3453307757593183,\n"," 3.34517041615768,\n"," 3.345010121346127,\n"," 3.3448498912800475,\n"," 3.3446897259148494,\n"," 3.344529625205997,\n"," 3.344369589108977,\n"," 3.344209617579347,\n"," 3.3440497105726785,\n"," 3.3438898680446276,\n"," 3.343730089950838,\n"," 3.343570376247062,\n"," 3.3434107268890347,\n"," 3.3432511418325728,\n"," 3.3430916210335275,\n"," 3.3429321644477765,\n"," 3.3427727720312785,\n"," 3.3426134437399964,\n"," 3.342454179529959,\n"," 3.34229497935722,\n"," 3.3421358431778962,\n"," 3.3419767709481487,\n"," 3.341817762624143,\n"," 3.3416588181621436,\n"," 3.341499937518422,\n"," 3.341341120649294,\n"," 3.3411823675111276,\n"," 3.341023678060336,\n"," 3.3408650522533585,\n"," 3.3407064900466956,\n"," 3.340547991396882,\n"," 3.3403895562604937,\n"," 3.340231184594152,\n"," 3.3400728763545153,\n"," 3.3399146314982997,\n"," 3.3397564499822296,\n"," 3.3395983317631197,\n"," 3.339440276797785,\n"," 3.3392822850430997,\n"," 3.3391243564559887,\n"," 3.3389664909933883,\n"," 3.338808688612328,\n"," 3.338650949269818,\n"," 3.3384932729229626,\n"," 3.338335659528876,\n"," 3.338178109044719,\n"," 3.3380206214277086,\n"," 3.3378631966350905,\n"," 3.3377058346241553,\n"," 3.3375485353522354,\n"," 3.337391298776693,\n"," 3.337234124854956]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["#theta = solver_with_ridge(X_train_standardized, y_train,alpha=0)\n","#print(theta)\n","\n","scores = []\n","\n","for alpha in np.arange(0,30.0,0.2):\n","\n","  theta = solver_with_ridge(X_train_standardized, y_train,alpha)\n","  rmse_score = rooted_mean_squared_error(X_test_standardized, y_test, theta)\n","  scores.append(rmse_score)\n","\n","scores"]},{"cell_type":"markdown","metadata":{"id":"ILjT1k5FpLVe"},"source":["Plot the graph of different scores here.  The resulting plot behaves in a different way based on the split training and test sets. Sometimes, the error just decreases or increases, but we can also see that the error decreases first, but after some point, it starts to increase. "]},{"cell_type":"code","execution_count":29,"metadata":{"id":"--Ky0LHO0wqM"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.plot(np.arange(0, 30, 0.2), scores)\n","plt.xlabel(\"alpha\")\n","plt.ylabel(\"RMSE\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S3nHFnagh0nC"},"source":["### 5. Model validation"]},{"cell_type":"markdown","metadata":{"id":"p7MnVQ5Rryof"},"source":["So far, we simply had one test set and one training set. Now the question is if those sets were enough to represent the dataset's distribution. To overcome this problem, various validation methods have been developed and used such as cross-validation or a repeated holdout test. Here, we will develop one function that performs the repeated holdout test. The key strategy of it is to create a completely different training and test set pair for each iteration. We need to iterate the holdout test that we performed many (k) times and return the average score.\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"M7JEfZjGijfH"},"outputs":[],"source":["\n","def repeated_hold_out(X, y, k, test_ratio):\n","  \"\"\"\n","  Input:\n","    - X: features\n","    - y: labels\n","    - test_ratio: ratio of the test set\n","  Output:\n","    - score: the average of k different test scores\n","\n","  \"\"\"\n","  scores = []\n","  for i in range(k):                                                                         #  Iterate k times to perform k validation processes.\n","    \n","     X_train, X_test, y_train, y_test = train_test_split(X,y,test_ratio)                     #  split the dataset into training and test sets with *random* indices\n","     X_train_standardized, X_test_standardized = apply_standardization(X_train, X_test)      #  Used *standardization* to fix the scale of the dataset\n","     \n","     theta = solver(X_train_standardized, y_train)                                           #  Fit the model with *solver* (without ridge) on the training set\n","    \n","     mae_score = mean_absolute_error(X_test_standardized, y_test, theta)    \n","     scores.append(mae_score)                                                                #  Save your *MAE* score into the list *scores*\n","  \n","  list_avg = np.mean(scores)                                                                 # After all the iterations, return the average of \n","  \n","  return list_avg"]},{"cell_type":"markdown","metadata":{"id":"Po5UX-gb2idF"},"source":["Run the holdout test on *X* and *y*, with k = 3 and test ratio = 20%."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"QuMGeSGfLjAL"},"outputs":[{"data":{"text/plain":["2.6807436108675238"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["holdout_score = repeated_hold_out(X, y, k = 3, test_ratio = 0.2) \n","holdout_score"]},{"cell_type":"markdown","metadata":{"id":"D7mSIvYbRoQq"},"source":["### 6. Put things together"]},{"cell_type":"markdown","metadata":{"id":"DWjft22LHEJm"},"source":["It's time to put everything we have done together here. Here we will create a function that manages whole process from receiving raw datasets to returning performance metrics, by modifying the repeated holdout function. This will help to manage the process clearly since it must contain all the functions we use for your dataset (Later we will replace it with scikit-learn's pipeline technique for the same purpose) - By having these management functions, we can switch off some of the techniques, add more techniques in the middle, or replace some of them with other methods, without any problem or confusion.\n","\n","* Completed `pipeline` following the instruction."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"6CU_kV0DRpYg"},"outputs":[],"source":["def pipeline(X, y, k = 5, test_ratio = 0.2, norm_method = \"standardization\", eval_method = \"RMSE\", alpha = 0):\n","  \"\"\"\n","  Input:\n","    - X: features\n","    - y: labels\n","    - test_ratio: ratio of the test set\n","  Output:\n","    - score: the average of k different test scores\n","\n","  \"\"\"\n","  for i in range(k):                                                                                # Iterate k times to perform k validation processes.\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(X,y,test_ratio)                             #  For each iteration, split the dataset into the training and test sets with *random* indices.\n","         \n","    if (norm_method == \"standardization\") :                                                         # Check the parameter *norm_method* if norm_method == standardization: - Use *standardization* to fix the scale of the dataset\n","                                                                                                        \n","      X_train_standardized, X_test_standardized = apply_standardization(X_train,X_test)\n","      theta = solver_with_ridge(X_train_standardized,y_train,alpha) \n","      if (eval_method == \"RMSE\") :                                                                  # Check the parameter \"eval_method\"\n","        scores = rooted_mean_squared_error(X_test_standardized,y_test, theta)\n","      elif(eval_method == \"MAE\"):\n","        scores = mean_absolute_error(X_test_standardized,y_test, theta)\n","      \n","        \n","    elif (norm_method == \"normalization\") :                                                         # if norm_method == normalization: - Use *normalization* to fix the scale of the dataset\n","       \n","      X_train_normalized, X_test_normalized = apply_normalization(X_train, X_test)\n","      theta = solver_with_ridge(X_train_normalized,y_train, alpha)\n","      if (eval_method == \"RMSE\") :                                                                  # Check the parameter \"eval_method\"\n","        scores = rooted_mean_squared_error(X_test_normalized,y_test, theta)\n","      elif(eval_method == \"MAE\"):\n","        scores = mean_absolute_error(X_test_normalized,y_test, theta)\n","      \n","    avg_scores = np.mean(scores)                                                                    # After all the iterations, return the average of *scores*.\n","\n","    \n","  return avg_scores"]},{"cell_type":"markdown","metadata":{"id":"P7GKrthoLOlD"},"source":["Now we are ready to run various tasks by using this single function. Will the best model the same under RMSE or MAE? Will different k or test ratio result in different best model? We can do many different trials to find a good model.\n","\n","- (Optional) Change a normalization method, an alpha value to find out the best classifier under either RMSE or MAE score."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"bkCeCDoPLNGp"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.381554187139335\n","3.6889707691157625\n","2.6552233130377147\n","3.4048585738846215\n","\n"," Best model average score:  2.667905136573565\n"]}],"source":["average_rmscores_st = pipeline(X,y,k=5, test_ratio=0.25,norm_method=\"standardization\", eval_method=\"RMSE\", alpha = 1 )\n","average_rmscores_nm = pipeline(X,y,k=5, test_ratio=0.25,norm_method=\"normalization\", eval_method=\"RMSE\", alpha = 1 )\n","\n","average_mascores_st = pipeline(X,y,k=5, test_ratio=0.25,norm_method=\"standardization\", eval_method=\"MAE\", alpha = 1 )\n","average_mascores_nm = pipeline(X,y,k=5, test_ratio=0.25,norm_method=\"normalization\", eval_method=\"MAE\", alpha = 1 )\n","\n","\n","print(average_rmscores_st)\n","print(average_rmscores_nm)\n","\n","print(average_mascores_st)\n","print(average_mascores_nm)\n","\n","\n","\n","best_model = average_mascores_st = pipeline(X,y,k=5, test_ratio=0.25,norm_method=\"standardization\", eval_method=\"MAE\", alpha = 1 )\n","print(\"\\n Best model average score: \", best_model)"]},{"cell_type":"markdown","metadata":{"id":"cWugHCgB2Cpq"},"source":["# END"]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
