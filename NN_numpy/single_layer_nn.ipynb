{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7vPaIoU4pqD"
      },
      "source": [
        "The neural network algorithm, by creating a model from scratch only with NumPy. Explains how forward/backpropagation and weight normalization/activation of the simple single-layer neural network work. Have used the famous MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtWBHOcoqnKH"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "VA351vjdly0R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "RANDOM_STATE = 12579"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCm_Zo2W8Q22"
      },
      "source": [
        "The MNIST dataset was constructed from two datasets of the US National Institute\n",
        "of Standards and Technology (NIST). The training dataset consists of handwritten\n",
        "digits from 250 different people, 50 percent high school students and 50 percent\n",
        "employees from the Census Bureau. Note that the test dataset contains handwritten digits from different people following the same split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPqJ7jTk7aaG"
      },
      "source": [
        "![alt text](https://dezyre.gumlet.net/images/Exploring+MNIST+Dataset+using+PyTorch+to+Train+an+MLP/MNIST+Dataset.png?w=900&dpr=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V29SZpPn8S69"
      },
      "source": [
        "This MNIST dataset can be directly downloaded via various routes including tensorflow dataset repository, keras dataset, and scikit-learn. This time, you can use `sklearn.datasets` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "CmAMpQXwdq4t"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aryad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUngFc88XsU"
      },
      "source": [
        "It has 70,000 different handwriting instances. It usually has 60,000 instances in the training set and the remaining ones in the test set, but scikit-learn loads it as a whole. We need to divide it into two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "gVuTqRrtlb44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object), array([6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958],\n",
            "      dtype=int64))\n",
            "[6903 7877 6990 7141 6824 6313 6876 7293 6825 6958]\n"
          ]
        }
      ],
      "source": [
        "#print(X.shape, y.shape)\n",
        "\n",
        "\n",
        "print(np.unique(y, return_counts = True))\n",
        "print(np.unique(y, return_counts = True)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kILkvez98bVl"
      },
      "source": [
        "We can also check the class distribution. Is it balanced or not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "YNAjbM-Z8ixF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "execution_count": 322,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.bar(np.unique(y, return_counts = True)[0], np.unique(y, return_counts = True)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEc2na5f9tRg"
      },
      "source": [
        "We may also need to apply normalization for better performance. Since it is clear that its maximum value is 255, we can simply normalize it by dividing the whole value by 255 (then the dataset will range from 0 to 1). We can use NumPy's broadcasting to divide the matrix by one scalar value. It is also possible to further standardize it to have a centeralized mean, but this time it is optional.\n",
        "- Apply normalization to X to have the range [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "GzXvdYtlmNqi"
      },
      "outputs": [],
      "source": [
        "X_normalized = X / 255 # CHANGE IT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmuNtw2d8dX"
      },
      "source": [
        "If we take a look at `y`, it has string labels! It might be disturbing when we need to handle them later, so let's also convert them to an integer form.\n",
        "- Change the type of the labels to integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "vfx0EL0wmoyU"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_integer = y.astype('int') # CHANGE IT\n",
        "#print(y_integer)\n",
        "#print(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqeW4C00l5se"
      },
      "source": [
        "Next, we need to split the dataset into two parts using scikit-learn's `train_test_split` method.\n",
        "- Use scikit-learn's `train_test_split` to create training and test sets.\n",
        "- Set **test_size** to 20%.\n",
        "- Enable stratification and shuffling.\n",
        "- use `X_normalized` and `y_integer`.\n",
        "- set `random_state` to the pre-defined variable `RANDOM_STATE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "SiERfT0gddu1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized , y_integer , test_size=0.2 , random_state=RANDOM_STATE) # CHANGE IT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcUJo3uj6sCa"
      },
      "source": [
        "Please print the mean and the standard deviation of `X_train` here (for validation purpose)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "1mujB3I56rjw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.13099379957340074, 0.30852164309770186)"
            ]
          },
          "execution_count": 326,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.mean(), X_train.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMldZza-faxL"
      },
      "source": [
        "Here we can check some of the instances that we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "TzRs98dodeAF",
        "outputId": "3a6fd962-3567-4616-e2c3-6e6116da3583"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
        "ax = ax.flatten()\n",
        "for i in range(10):\n",
        "  img = X_train[y_train == i][0].reshape(28, 28)\n",
        "  ax[i].imshow(img, cmap='Greys')\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baT1IBRFqrBJ"
      },
      "source": [
        "## 2. Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVROBu9_py1h"
      },
      "source": [
        "First, we need to fix the labels, which are now in a sparse form, to have a one-hot encoded form for better computation of error terms using NumPy's vectorization. Let's create a function that receives a label vector and transform it into a one-hot encoded label matrix.\n",
        "\n",
        "- Complete the `one_hot` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "msvnKGsR0yUS"
      },
      "outputs": [],
      "source": [
        "def one_hot(y):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - y : set of labels\n",
        "\n",
        "  Output:\n",
        "    - onehot: a one-hot-encoded array\n",
        "\n",
        "  This function creates an one-hot encoded representation of the labels.\n",
        "  This means that you will have a set of binary columns indicading each possible class.\n",
        "\n",
        "  You have to develop this one hot encoding strategy without using Python for loop\n",
        "\n",
        "  Expected outcome:\n",
        "    one_hot(np.array([1,0,2,3]))\n",
        "\n",
        "    array([[0., 1., 0., 0.],\n",
        "          [1., 0., 0., 0.],\n",
        "          [0., 0., 1., 0.],\n",
        "          [0., 0., 0., 1.]])\n",
        "  \"\"\"\n",
        "  \n",
        "  n_classes = np.max(y) + 1\n",
        "  y_one_hot = np.zeros((len(y),n_classes))\n",
        "  y_one_hot[np.arange(len(y)), y] = 1\n",
        "  \n",
        "  return y_one_hot # CHANGE IT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dAPaf1708Bs"
      },
      "source": [
        "Test code here "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "sGFbZ_tN00WK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "execution_count": 329,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_hot(np.array([1,0,2,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7niKihIdpYa5"
      },
      "source": [
        "Next, we may also need a sigmoid function for the output values as we are dealing with a classification problem. Sigmoid can be represented as follows:\n",
        "\n",
        "$$ h_ \\theta (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }  $$\n",
        "\n",
        "\n",
        "- Complete the sigmoid function below that supports both vectors and scalars (this can be automatically handled if you use NumPy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "id": "hFSCiidT1Drz"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - z: input vector or scalar value\n",
        "\n",
        "  Output:\n",
        "    - sigmoid: output sigmoid-transformed vector or scalar value\n",
        "\n",
        "  Calculate the sigmoid value of the input.\n",
        "\n",
        "  Expected outcome:\n",
        "    sigmoid(np.array([np.inf, -np.inf, 0]))\n",
        "\n",
        "    array([1. , 0. , 0.5])\n",
        "  \"\"\"\n",
        "  sig_func = 1 / (1 + np.exp(-x))\n",
        "  \n",
        "  \n",
        "  return sig_func # CHANGE IT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlugwi8r0O-H"
      },
      "source": [
        "Test the code here "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "3fJNnW22qMQ0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1. , 0. , 0.5])"
            ]
          },
          "execution_count": 331,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sigmoid(np.array([np.inf, -np.inf, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnfdCK6Fqtls"
      },
      "source": [
        "## 3. Our FCN classifier with the class structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2YtPQk0pXK"
      },
      "source": [
        "Now it is time to create our neural network model from scratch! we eventually need to integrate everything into scikit-learn's pipeline, so it's important to have an appropriate class structure. To do this, we may need to extend `BaseEstimator` and `TransformerMixin` to make scikit-learn recognize that our class is a valid classifier.\n",
        "\n",
        "We are going to develop a neural network with one layer for simplicity. That means we will have two different sets of weights.\n",
        "\n",
        "- First layer: [input size (number of features), hidden layer size]\n",
        "- Second layer (or output layer): [hidden layer size, output size (number of classes)]\n",
        "\n",
        "In the class structure `FullyConnectedNetwork` below, we will develop five different methods as follows:\n",
        " - `compile`: Given parameters, this function will initialize weight and bias values needed for our neural network model.\n",
        "   - Here, you will initialize bias and weights based on a chosen initialization technique.\n",
        "        - We need to implement three different options: normal, Xavier, and he\n",
        "        - Each technique initializes the weight using the normal distribution but different standard deviation. The mean value remains the same.\n",
        "          - Normal:\n",
        "$ \\mu = 0, \\sigma = 0.1 $\n",
        "          - Xavier:\n",
        "$ \\mu = 0, \\sigma = \\sqrt{\\frac{2}{n_{in} + n_{out}}}$\n",
        "          - He:\n",
        "$ \\mu = 0, \\sigma = \\sqrt{\\frac{2}{n_{in}}}$\n",
        " - `forward`: Perform a forward propagation with the weights saved in the model.\n",
        " - `back_propagation`: Perform a back propagation (training the model).\n",
        "    - Most of the derivative terms are already provided. We will only need to finish some part of it.\n",
        "      - Weight and bias update\n",
        "      - Derivative of the sigmoid function\n",
        "        - $σ(x)=σ(x)(1−σ(x))$.\n",
        "\n",
        " - `fit`: Run the whole fitting process (forward and backpropagation for each batch).\n",
        " - `cost`: Calculate the cost (cross-entropy) together with the elastic net (l1/l2). The fomula is described in the function.\n",
        " - `predict`: With a trained model, perform a prediction of unseen data by running the forward propagation with the trained weight and bias.\n",
        " - `evaluate`: With trained weight and bias, perform a prediction of test data and calculate the performance metric (in our case, those are training and validation accuracy scores)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "UxEdUv9hmiTd"
      },
      "outputs": [],
      "source": [
        "class FullyConnectedNetwork(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_hidden=30, l2=0., l1=0., epochs=100, eta=0.001, validation_rate = 0.3,\n",
        "                 shuffle=True, batch_size=1, init_technique = \"normal\", seed=None, debug=True):\n",
        "\n",
        "        \"\"\"\n",
        "        The class structure receive the following parameters to construct and test the model:\n",
        "\n",
        "        Input:\n",
        "          - n_hidden: Number of hidden nodes.\n",
        "          - l2: Lambda value for L2-regularization.\n",
        "          - l1: Lambda value for L1-regularization.\n",
        "          - epochs: Number of passes over the training set.\n",
        "          - eta: Learning rate.\n",
        "          - validation_rate: size of the validation set.\n",
        "          - shuffle: Enabling shuffling option of the dataset every epoch.\n",
        "          - batch_size: Number of training examples per batch.\n",
        "          - init_technique: Indicator for an initialization technique.\n",
        "          - seed: Random seed for initializing weights and shuffling.\n",
        "        \"\"\"\n",
        "        self.seed = seed\n",
        "\n",
        "        # DEFINE RANDOM NUMBER GENERATOR USING THE INPUT SEED\n",
        "    \n",
        "        self.random = np.random.default_rng(seed) # CHANGE IT\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.l2 = l2\n",
        "        self.l1 = l1\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.validation_rate = validation_rate\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.debug = debug\n",
        "        self.init_technique = init_technique\n",
        "\n",
        "    def compile(self, n_features, n_outputs):\n",
        "        \"\"\"\n",
        "        Initializing the weights of the model\n",
        "\n",
        "        - Here we will initialize bias and weights based on chosen initialization technique.\n",
        "        - The classifier has three different options: normal, xavier, and he\n",
        "        - Each technique initializes the weight using the normal distribution but different standard deviation.\n",
        "        - Use self.init_technique to check the chosen technique and use self.random to perform the sampling.\n",
        "\n",
        "        Input:\n",
        "          - n_features: input size of the network\n",
        "          - n_outputs: output size of the network\n",
        "          - Unit size of the layer is given as self.n_hidden\n",
        "\n",
        "        Steps:\n",
        "          1. Check if self.random using NumPy's random number generator is created.\n",
        "             We will use this generator throughout this function.\n",
        "          2. Create lists self.W and self.B which will keep the weight values for each layer.\n",
        "          3. Set mean and standard deviation for different initialization technique.\n",
        "          4. Create weights and bias for the linkage between inputs and the first layer.\n",
        "            - Weight should have the size [n_features, self.n_hidden].\n",
        "            - Bias should have the size [self.n_hidden].\n",
        "            - Weight initialization should be applied to the weights only.\n",
        "            - Bias should be initizalied by zeros.\n",
        "          5. Create weights and bias for the linkage between the first layer and the output layer.\n",
        "            - Weight should have the size [self.n_hidden, n_outputs].\n",
        "            - Bias should have the size [n_outputs].\n",
        "            - Weight initialization should be applied to the weights only.\n",
        "            - Bias should be initizalied by zeros.\n",
        "          5. Save the weights to self.W and biases to self.B. Each list should have two elements for each layer.\n",
        "        \"\"\"\n",
        "        if self.random is None:\n",
        "          raise ValueError(\" Random generator not initialized, use np.random.default_rng \")\n",
        "        \n",
        "        self.B = [] \n",
        "        self.W = [] \n",
        "        \n",
        "        if self.init_technique == 'normal' :\n",
        "          mean = 0\n",
        "          std_deviation = 0.1\n",
        "          \n",
        "        elif self.init_technique == 'Xavier' :\n",
        "          mean = 0\n",
        "          std_deviation = np.sqrt(2 / (n_features + n_outputs))\n",
        "        \n",
        "        elif self.init_technique == 'he' :\n",
        "          mean = 0\n",
        "          std_deviation = np.sqrt(2 / n_features)\n",
        "          \n",
        "        else :\n",
        "          raise ValueError(\"Invalid initializing technique\")\n",
        "\n",
        "        # 1. Creating weights and bias for [input -> hidden]\n",
        "        # Using specific initialization techniques for weights\n",
        "        # Weights have the size (n_features, self.n_hidden)\n",
        "        # Using np.zeros for bias with the size 'self.n_hidden'\n",
        "\n",
        "        b_h = np.zeros(self.n_hidden) \n",
        "        w_h = np.random.normal(mean, std_deviation, (n_features, self.n_hidden)) \n",
        "\n",
        "        # 2. Append bias to self.B and weights to self.W\n",
        "        \n",
        "        self.B.append(b_h)\n",
        "        self.W.append(w_h)\n",
        "       \n",
        "        # 3. Creating weights and bias for [hidden -> output]\n",
        "        # Using specific initialization techniques for weights\n",
        "        # Weights should have the size (self.n_hidden, n_outputs)\n",
        "        # Use np.zeros for bias with the size 'n_outputs'\n",
        "\n",
        "        b_out = np.zeros(n_outputs) \n",
        "        w_out = np.random.normal(mean, std_deviation, (self.n_hidden, n_outputs))\n",
        "\n",
        "        # 4. Append bias to self.B and weights to self.W\n",
        "        \n",
        "        self.B.append(b_out)\n",
        "        self.W.append(w_out)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Given the dataset X, compute forward propagation step with the weights and bias saved in the list.\n",
        "        This process eventually outputs ten numbers in our case as the dataset has ten outputs.\n",
        "        Forward propagation is performed by multiple chained dot products of inputs and weights.\n",
        "\n",
        "        Input:\n",
        "          - X: features\n",
        "\n",
        "        Output:\n",
        "          - Z: Result of dot product of the weights and the previous output for each phase\n",
        "          - A: A list that contains sigmoided values of A\n",
        "        Steps:\n",
        "          1. Create two lists Z and A.\n",
        "          2. Take a dot product of X and the first weight self.W[0] - save the result into Z\n",
        "          3. Apply sigmoid function to the first Z - save the result into A\n",
        "          4. Take a dot product of A and the second weight self.W[1] - save the result into Z\n",
        "          5. Apply sigmoid function to the second Z - save the result into A\n",
        "          6. Return Z and A\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Z = [] \n",
        "        A = [] \n",
        "\n",
        "        # Step 1: net input of hidden layer\n",
        "        # - We are calculating the first XW+b.\n",
        "        # - Taking a dot product of the input features and the initial weights.\n",
        "        # - Adding the outcome to list Z.\n",
        "        \n",
        "        first_Z = X.dot(self.W[0]) + self.B[0]\n",
        "        Z.append(first_Z) \n",
        "\n",
        "        # Step 2: activation of hidden layer\n",
        "        # - Apply the sigmoid function to the dot producted outcome.\n",
        "        # - Add the outcome to list A.\n",
        "        sigmoid_A1 = sigmoid(first_Z)\n",
        "        A.append(sigmoid_A1)\n",
        "\n",
        "        # Step 3: net input of output layer\n",
        "        # - We are calculating the second XW+b.\n",
        "        # - Take a dot product of the intermediate features and the weights of the output layer.\n",
        "        # - Add the outcome to list Z.\n",
        "        second_Z = sigmoid_A1.dot(self.W[1]) + self.B[1]\n",
        "        Z.append(second_Z) \n",
        "\n",
        "        # Step 4: activation output layer\n",
        "        # - Apply the sigmoid function to the dot producted outcome.\n",
        "        # - For simplicity, here the network uses sigmoid instead of softmax.\n",
        "        # - Add the outcome to list A.\n",
        "        sigmoid_A2 = sigmoid(second_Z)\n",
        "        A.append(sigmoid_A2)\n",
        "\n",
        "        return Z, A\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "\n",
        "        Predict class labels by performing forward propagation.\n",
        "\n",
        "        Input:\n",
        "          - X: Feature matrix.\n",
        "        Output:\n",
        "          - y_pred: Predicted class labels for all data instances.\n",
        "\n",
        "        Steps:\n",
        "          1. Run forward proparation on X and get Z, a.\n",
        "          2. Calculate y_pred by using the final output (A[-1]) and with np.argmax\n",
        "            - We have to choose the index of the one with the highest value, which means the highest probability.\n",
        "          3. Return the prediction. We SHOULD perform the operation using NumPy's vectorization feature.\n",
        "             This means that if we put many instances at once as an input, this function should calculate the result also at once.\n",
        "\n",
        "        \"\"\"\n",
        "        Z , A = self.forward(X) \n",
        "      \n",
        "        y_pred = np.argmax(A[-1], axis = 1) \n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def cost(self, y_truth, y_pred):\n",
        "        \"\"\"\n",
        "\n",
        "        This function computes the cost for the classification task.\n",
        "        The network supports Elastic net (combination of l1 and l2 with corresponding weights).\n",
        "\n",
        "        Input:\n",
        "          - y_truth: \"One-hot encoded\" class labels.\n",
        "          - y_pred: Activation of the output layer (= output of the forward propagation function).\n",
        "          - The weights for l1 and l2 are saved into self.l1 and self.l2.\n",
        "\n",
        "        Output:\n",
        "          - cost: Regularized cost\n",
        "\n",
        "        Steps:\n",
        "          1. Calculate the cross entropy between the truth (y) and predicted values (y*).\n",
        "             - y * log(y*) - (1 - y) * log(1 - y*)\n",
        "          2. Add l1 and l2 terms to the cost.\n",
        "            - L1 term is the sum of absolute weight values.\n",
        "            - L2 term is the sum of squared weight values.\n",
        "            - We should multiply l1 and l2 ratio saved in self.l1 and self.l2 (this will decide the degree of regularization).\n",
        "            - We should NOT include weights that belong to the bias values.\n",
        "          3. Return the total cost (cross entropy + L1 term + L2 term).\n",
        "\n",
        "        \"\"\"\n",
        "        if len(y_truth.shape) == 1:\n",
        "          y_truth = one_hot(y_truth)\n",
        "          \n",
        "        entropy =  - np.mean(y_truth * np.log(y_pred) + (1 - y_truth) * np.log(1 - y_pred))\n",
        "        \n",
        "        L1_term = 0\n",
        "        L2_term = 0\n",
        "        for w in self.W: \n",
        "            L1_term += np.sum(np.abs(w))\n",
        "            L2_term += np.sum(w**2)\n",
        "        \n",
        "        L1_term *= self.l1 \n",
        "        L2_term *= self.l2     \n",
        "          \n",
        "        cost = entropy + L1_term + L2_term\n",
        "\n",
        "        return cost\n",
        "\n",
        "\n",
        "\n",
        "    def back_propagation(self, X_train, batch_idx, A, y_truth):\n",
        "      \"\"\"\n",
        "      Performing back propagation based on the result of forward propagation and true labels (for each batch).\n",
        "\n",
        "      Input:\n",
        "        X_train: Training features.\n",
        "        batch_idx: The current batch indices from the fit function.\n",
        "        A: Sigmoided output values - the result of forward propagation.\n",
        "        y_truth: One-hot encoded true labels.\n",
        "\n",
        "      Output:\n",
        "        None\n",
        "        Update the weights and biases in self.W/self.B\n",
        "      \"\"\"\n",
        "  \n",
        "\n",
        "      # OUTPUT WEIGHTS (LAYER-OUTPUT)\n",
        "\n",
        "      #δC/δA * δA/δZ\n",
        "      delta_out = A[-1] - y_truth[batch_idx]\n",
        "      # δC/δA * δA/δZ * δZ/δW\n",
        "      grad_w_out = np.dot(A[-2].T, delta_out)\n",
        "      # δC/δA * δA/δZ * δZ/δB\n",
        "      grad_b_out = np.sum(delta_out, axis=0)\n",
        "      \n",
        "      # Using the final gradients of the weight and bias (grad_w_out, grad_b_out), the network needs to update its current weight values.\n",
        "      # The gradient of w and b are already calculated and all we need to do is to merge it with l1/l2 terms.\n",
        "      # Change the values of self.W[1], self.B[1] (output weight and bias).\n",
        "      # - We should also apply l1 and l2 normalization to the weight (not to the bias).\n",
        "      # - We should use the learning rate (self.eta) when changing the value.\n",
        "\n",
        "      delta_w_out = grad_w_out / len(batch_idx)  \n",
        "      delta_b_out = grad_b_out / len(batch_idx)  \n",
        "      self.W[1] -= self.eta * (delta_w_out + self.l1 * np.sign(self.W[1]) + self.l2 * self.W[1])  \n",
        "      self.B[1] -= self.eta * delta_b_out  \n",
        "\n",
        "      # HIDDEN WEIGHTS (INPUT-LAYER)\n",
        "\n",
        "      # To continue to take derivatives backwards, we need to take a derivative of the sigmoid function.\n",
        "      # Here we are trying to take derivative of a sigmoided output A[0].\n",
        "      # Derivative of sigmoid σ(x) can be represented as σ(x)(1−σ(x)).\n",
        "\n",
        "      sigmoid_derivative_h = A[0] * (1- A[0]) \n",
        "      delta_h = (np.dot(delta_out, self.W[1].T) * sigmoid_derivative_h)\n",
        "      grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n",
        "      grad_b_h = np.sum(delta_h, axis=0)\n",
        "      \n",
        "      # Using the final gradients of the weight and bias (grad_w_h, grad_b_h).\n",
        "      # The gradient of w and b are already calculated and all we need to do is to merge it with l1/l2 terms.\n",
        "      # Change the values of self.W[0], self.B[0] (output weight and bias).\n",
        "      # - We should also apply l1 and l2 normalization to the weight (not to the bias).\n",
        "      # - We should use the learning rate (self.eta) when changing the value.\n",
        "\n",
        "      delta_w_h = grad_w_h / len(batch_idx) \n",
        "      delta_b_h = grad_b_h / len(batch_idx) \n",
        "\n",
        "      self.W[0] -= self.eta * (delta_w_h + self.l1 * np.sign(self.W[0]) + self.l2 * self.W[0]) \n",
        "      self.B[0] -= self.eta * delta_b_h  \n",
        "\n",
        "\n",
        "    def evaluate(self, epoch, X_train, X_valid, y_train, y_valid):\n",
        "      \"\"\"\n",
        "      Evaluate performances on the training and validation sets per epoch\n",
        "\n",
        "      Input:\n",
        "        - epoch: Current epoch number.\n",
        "        - X_train: Training features\n",
        "        - X_valid: Validation features\n",
        "        - y_train: Training labels\n",
        "        - y_valid: Validation labels\n",
        "\n",
        "      Output:\n",
        "        - None\n",
        "        Append the cost and performance metrics of current epoch to self.history\n",
        "      \"\"\"\n",
        "\n",
        "      # Step 1. Calling self.forward on X_train to calculate the output with current weights and bias of the model.\n",
        "      Z, A = self.forward(X_train) \n",
        "      \n",
        "      #Z_valid, A_valid = self.forward(X_valid)\n",
        "      #print(\"y_pred size\", len(A_train))\n",
        "      \n",
        "      # Step 2. calling predict functions with both X_train and X_valid and save the predicted values accordingly.\n",
        "      y_train_pred = self.predict(X_train)\n",
        "      y_valid_pred = self.predict(X_valid)\n",
        "      \n",
        "\n",
        "      # Step 2. Calling self.cost with one hot encoded y_train and the probability of the output prediction.\n",
        "      y_train_enc = one_hot(y_train)\n",
        "      y_valid_enc = one_hot(y_valid)\n",
        "      \n",
        "     # print(\"y_train_enc.size\", y_train_enc.size)\n",
        "      #print(y_train_pred.size)\n",
        "      \n",
        "      cost = self.cost(y_train_enc , A[1]) \n",
        "\n",
        "      # Step 4. Calculate accuracy scores.\n",
        "      # - between y_train_pred and y_train.\n",
        "      # - between y_valid_pred and y_valid.\n",
        "      train_acc = np.mean(y_train_pred == y_train) \n",
        "      valid_acc = np.mean(y_valid_pred == y_valid) \n",
        "\n",
        "      # Step 5. Saving the results into the dictionary.\n",
        "      \n",
        "      if self.debug == True:\n",
        "        print('%d/%d | Cost: %.2f '\n",
        "                        '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
        "                        (epoch+1, self.epochs, cost,\n",
        "                          train_acc*100, valid_acc*100))\n",
        "\n",
        "      self.history['cost'].append(cost)\n",
        "      self.history['train_acc'].append(train_acc)\n",
        "      self.history['valid_acc'].append(valid_acc)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "\n",
        "        Learn weights from training data.\n",
        "\n",
        "        Input\n",
        "          - X: features (training+validation)\n",
        "          - y: labels\n",
        "\n",
        "        Output\n",
        "          - self.history: information about cost and accuracy scores\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.history = {'cost': [], 'train_acc': [], 'valid_acc': []}\n",
        "\n",
        "        # Step 1: Select different training and test sets. Using scikit-learn's train_test_split.\n",
        "        # Turn on the stratification option and use self.validation_rate\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=self.validation_rate, stratify=y) \n",
        "        \n",
        "        # Step 2: Compile (initialize) the parameters by running self.compile with correct number of features and outputs\n",
        "        self.compile(X_train.shape[1], np.unique(y_train).shape[0])\n",
        "        \n",
        "        # Step 3: Prepare one-hot encoded training labels by using one_hot function on y_train\n",
        "        y_train_enc = one_hot(y_train) \n",
        "\n",
        "        # Step 4: iterate over training epochs\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            # Step 5: setthing the indices\n",
        "            # - if self.shuffle is True, shuffle the indices using self.random.shuffle or permutation\n",
        "            indices = np.arange(X_train.shape[0]) \n",
        "            \n",
        "            if self.shuffle:\n",
        "                self.random.shuffle(indices)\n",
        "                \n",
        "            # Step 6: iterate over the data\n",
        "            # - For each iteration, we need to choose the data\n",
        "            for start_idx in range(0, indices.shape[0] , self.batch_size):\n",
        "                end_idx = min(start_idx + self.batch_size, indices.shape[0])\n",
        "                batch_idx = indices[start_idx:end_idx]\n",
        "\n",
        "                if len(batch_idx) == 0:\n",
        "                    continue \n",
        "                # Step 7: Run a forward propagation\n",
        "                Z_train, A_train = self.forward(X_train[batch_idx])\n",
        "                \n",
        "                # Step 8: Run back propagation\n",
        "                # - Using X_train, batch_idx, A, and y_train_enc\n",
        "               \n",
        "                self.back_propagation(X_train, batch_idx, A_train, y_train_enc)\n",
        "\n",
        "            # call evaluate function after inner loop (whole batch cycles) is complete\n",
        "            self.evaluate(i, X_train, X_valid, y_train, y_valid)\n",
        "\n",
        "        # Step 9: After all loops are complete, return self.history\n",
        "        return self.history\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "     \n",
        "      return self.history\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "      \n",
        "      y_pred = self.predict(X)\n",
        "      acc = np.sum(y == y_pred) / X.shape[0]\n",
        "      return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QAzKAjhhwHu"
      },
      "source": [
        "After you finish developing the methods in the class structure, you can create a new instance by calling 'FullyConnectedNetwork' class. Create your model using the following parameters:\n",
        "\n",
        "- n_hidden = 150\n",
        "- l2 = 0.01\n",
        "- epochs = 30\n",
        "- eta = 0.001\n",
        "- batch_size = 50\n",
        "- shuffle = True\n",
        "- seed = `RANDOM_STATE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "id": "pEkq3fydrmVw"
      },
      "outputs": [],
      "source": [
        "nn = FullyConnectedNetwork(n_hidden=150, l2=0.01, epochs=30, eta=0.001, batch_size=50, shuffle=True, seed=RANDOM_STATE) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXBBoj_oh5Vt"
      },
      "source": [
        "Then the fit methods will run the model for 30 epochs\n",
        " - Fit the network on `X_train` and `y_train` and save the output to `history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "id": "uSKfzn3czNrU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30732800\n",
            "13171200\n",
            "39200\n",
            "16800\n",
            "1/30 | Cost: 12.07 | Train/Valid Acc.: 31.15%/31.01% \n",
            "2/30 | Cost: 11.88 | Train/Valid Acc.: 45.56%/45.47% \n",
            "3/30 | Cost: 11.70 | Train/Valid Acc.: 53.81%/53.86% \n",
            "4/30 | Cost: 11.52 | Train/Valid Acc.: 59.80%/59.97% \n",
            "5/30 | Cost: 11.35 | Train/Valid Acc.: 61.97%/61.85% \n",
            "6/30 | Cost: 11.18 | Train/Valid Acc.: 65.96%/65.78% \n",
            "7/30 | Cost: 11.02 | Train/Valid Acc.: 67.50%/67.27% \n",
            "8/30 | Cost: 10.86 | Train/Valid Acc.: 69.20%/68.97% \n",
            "9/30 | Cost: 10.71 | Train/Valid Acc.: 70.73%/70.55% \n",
            "10/30 | Cost: 10.56 | Train/Valid Acc.: 72.25%/72.12% \n",
            "11/30 | Cost: 10.41 | Train/Valid Acc.: 73.17%/73.12% \n",
            "12/30 | Cost: 10.26 | Train/Valid Acc.: 73.66%/73.73% \n",
            "13/30 | Cost: 10.12 | Train/Valid Acc.: 74.19%/74.28% \n",
            "14/30 | Cost: 9.98 | Train/Valid Acc.: 75.35%/75.18% \n",
            "15/30 | Cost: 9.85 | Train/Valid Acc.: 75.85%/75.77% \n",
            "16/30 | Cost: 9.72 | Train/Valid Acc.: 76.59%/76.42% \n",
            "17/30 | Cost: 9.59 | Train/Valid Acc.: 77.02%/76.91% \n",
            "18/30 | Cost: 9.46 | Train/Valid Acc.: 77.60%/77.38% \n",
            "19/30 | Cost: 9.33 | Train/Valid Acc.: 78.00%/77.87% \n",
            "20/30 | Cost: 9.21 | Train/Valid Acc.: 78.47%/78.33% \n",
            "21/30 | Cost: 9.09 | Train/Valid Acc.: 78.62%/78.47% \n",
            "22/30 | Cost: 8.97 | Train/Valid Acc.: 79.15%/79.05% \n",
            "23/30 | Cost: 8.86 | Train/Valid Acc.: 79.43%/79.35% \n",
            "24/30 | Cost: 8.74 | Train/Valid Acc.: 79.90%/79.86% \n",
            "25/30 | Cost: 8.63 | Train/Valid Acc.: 80.05%/79.93% \n",
            "26/30 | Cost: 8.52 | Train/Valid Acc.: 80.57%/80.39% \n",
            "27/30 | Cost: 8.41 | Train/Valid Acc.: 80.93%/80.79% \n",
            "28/30 | Cost: 8.30 | Train/Valid Acc.: 80.83%/80.73% \n",
            "29/30 | Cost: 8.19 | Train/Valid Acc.: 81.11%/80.95% \n",
            "30/30 | Cost: 8.09 | Train/Valid Acc.: 81.46%/81.22% \n"
          ]
        }
      ],
      "source": [
        "history = nn.fit(X_train, y_train) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGuc6mti3lIp"
      },
      "source": [
        "After the training is done, we should be able to plot the training and validation accuracy scores over time using the `history` dictionary returned by the fit function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "id": "n79B9p-OzPIA"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['valid_acc'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPA_LeMzqyFc"
      },
      "source": [
        "## 4. Integrate our classifier into the scikit-learn pipeline and the randomized search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP7V3OjInmQM"
      },
      "source": [
        "This time, we can use the same strategy as the second lab, trying to integrate everything from the raw dataset to the performance metrics into scikit-learn pipeline.\n",
        "\n",
        "- Task 1: Create a `Normalizer` class that extends BaseEstimator and TransformerMixin.\n",
        " -  normalizer should do the following job:\n",
        "   - Normalizer: To make the features have the range [0, 1] and also **center the points to zero by subtracting 0.5 from the values.**\n",
        "   - Using NumPy's broadcasting to calculate (X / 255) - 0.5.\n",
        "\n",
        "\n",
        "- Task 2: Create a pipeline that integrates both normalizer and neural network classifier.\n",
        "  - The pipeline should contain the following modules.\n",
        "    - 'normalizer': Normalizer class\n",
        "    - 'classifier': `FullyConnectedNetwork` with default parameters but with epochs=10.\n",
        "\n",
        "- Task 3: Fit the pipeline on the datasets (`X_train`, `y_train`).\n",
        "  - We should **not** use `X_normalized` this time as the normalizer is now part of the pipeline. This means we might need to split the dataset again with `train_test_split` by using `X` and `y_integer`. Turn on stratification, and set `test_size` = 20%.\n",
        "  - Fit the pipeline and report the test score on `X_test` and `y_test` to `pipeline_score`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "id": "7KQyD1cf1Na8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30732800\n",
            "13171200\n",
            "39200\n",
            "16800\n",
            "1/10 | Cost: 0.18 | Train/Valid Acc.: 78.23%/77.79% \n",
            "2/10 | Cost: 0.11 | Train/Valid Acc.: 86.23%/85.75% \n",
            "3/10 | Cost: 0.09 | Train/Valid Acc.: 88.48%/88.01% \n",
            "4/10 | Cost: 0.08 | Train/Valid Acc.: 89.55%/89.28% \n",
            "5/10 | Cost: 0.07 | Train/Valid Acc.: 90.26%/89.78% \n",
            "6/10 | Cost: 0.07 | Train/Valid Acc.: 90.77%/90.50% \n",
            "7/10 | Cost: 0.06 | Train/Valid Acc.: 91.27%/91.04% \n",
            "8/10 | Cost: 0.06 | Train/Valid Acc.: 91.40%/91.03% \n",
            "9/10 | Cost: 0.06 | Train/Valid Acc.: 91.90%/91.60% \n",
            "10/10 | Cost: 0.05 | Train/Valid Acc.: 92.22%/91.76% \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;normalizer&#x27;, Normalizer()),\n",
              "                (&#x27;classifier&#x27;, FullyConnectedNetwork(epochs=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;normalizer&#x27;, Normalizer()),\n",
              "                (&#x27;classifier&#x27;, FullyConnectedNetwork(epochs=10))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Normalizer</label><div class=\"sk-toggleable__content\"><pre>Normalizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FullyConnectedNetwork</label><div class=\"sk-toggleable__content\"><pre>FullyConnectedNetwork(epochs=10)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('normalizer', Normalizer()),\n",
              "                ('classifier', FullyConnectedNetwork(epochs=10))])"
            ]
          },
          "execution_count": 358,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Normalizer(BaseEstimator, TransformerMixin):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "        \n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    X_normalized = X / 255 - 0.5\n",
        "    return X_normalized\n",
        "      \n",
        "pipe = Pipeline([\n",
        "    ('normalizer', Normalizer()),\n",
        "    ('classifier', FullyConnectedNetwork(epochs=10))\n",
        "])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_integer, test_size=0.2, stratify=y_integer)\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "id": "tQRQASa5bz89"
      },
      "outputs": [],
      "source": [
        "pipeline_score = pipe.score(X_test, y_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "Lcykfo9x6zY8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9176428571428571\n"
          ]
        }
      ],
      "source": [
        "# PRINT THE SCORE HERE\n",
        "print(pipeline_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMdqSR8k4hRI"
      },
      "source": [
        "- Task 4: Randomized search\n",
        "  - After constructing your pipeline, you can perform a randomized search on it.\n",
        "  - Define your parameter grid with the following information. Use `np.arange` if neccesary:\n",
        "    - l1 of classifier: 0 to 0.01 (included) with step size 0.002.\n",
        "    - l2 of classifier: 0 to 0.01 (included) with step size 0.002.\n",
        "    - size of hidden layer of classifier: 20 to 100 with step size 10\n",
        "    - learning rate of classifier: 0.0001 to 0.001 with step size 0.0001\n",
        "    - initialization techniques of classifier: [normal, xavier, he]\n",
        "  - Run your randomized search with cv=3. Fit it on your previous `X_train` and `y_train`.\n",
        "    - Make 10 different attempts.\n",
        "    - Set `random_state` = `RANDOM_STATE`\n",
        "  - Report your best classifier and best score into the variables `best_classifier` and `best_score`.\n",
        "\n",
        "  - **Note that this task will take a few hours based on computing power, so you may not need to finish the run. The submission is regarded correct if the logic is correct - if you cannot finish the task before the submission.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "rysn4lmhmpBb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 3.49 | Train/Valid Acc.: 11.27%/11.29% \n",
            "2/10 | Cost: 1.12 | Train/Valid Acc.: 11.19%/11.20% \n",
            "3/10 | Cost: 0.67 | Train/Valid Acc.: 11.19%/11.20% \n",
            "4/10 | Cost: 0.60 | Train/Valid Acc.: 11.19%/11.20% \n",
            "5/10 | Cost: 0.58 | Train/Valid Acc.: 11.19%/11.20% \n",
            "6/10 | Cost: 0.56 | Train/Valid Acc.: 11.19%/11.20% \n",
            "7/10 | Cost: 0.55 | Train/Valid Acc.: 11.19%/11.20% \n",
            "8/10 | Cost: 0.54 | Train/Valid Acc.: 11.19%/11.20% \n",
            "9/10 | Cost: 0.52 | Train/Valid Acc.: 11.19%/11.20% \n",
            "10/10 | Cost: 0.51 | Train/Valid Acc.: 11.19%/11.20% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 3.47 | Train/Valid Acc.: 12.06%/12.20% \n",
            "2/10 | Cost: 1.12 | Train/Valid Acc.: 11.30%/11.30% \n",
            "3/10 | Cost: 0.67 | Train/Valid Acc.: 11.30%/11.30% \n",
            "4/10 | Cost: 0.60 | Train/Valid Acc.: 11.30%/11.30% \n",
            "5/10 | Cost: 0.58 | Train/Valid Acc.: 11.30%/11.30% \n",
            "6/10 | Cost: 0.56 | Train/Valid Acc.: 11.30%/11.30% \n",
            "7/10 | Cost: 0.55 | Train/Valid Acc.: 11.30%/11.30% \n",
            "8/10 | Cost: 0.54 | Train/Valid Acc.: 11.30%/11.30% \n",
            "9/10 | Cost: 0.52 | Train/Valid Acc.: 11.30%/11.30% \n",
            "10/10 | Cost: 0.51 | Train/Valid Acc.: 11.30%/11.30% \n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "1/10 | Cost: 3.50 | Train/Valid Acc.: 11.53%/11.45% \n",
            "2/10 | Cost: 1.13 | Train/Valid Acc.: 11.26%/11.27% \n",
            "3/10 | Cost: 0.67 | Train/Valid Acc.: 11.26%/11.27% \n",
            "4/10 | Cost: 0.60 | Train/Valid Acc.: 11.26%/11.27% \n",
            "5/10 | Cost: 0.58 | Train/Valid Acc.: 10.44%/10.44% \n",
            "6/10 | Cost: 0.56 | Train/Valid Acc.: 10.44%/10.44% \n",
            "7/10 | Cost: 0.55 | Train/Valid Acc.: 11.26%/11.27% \n",
            "8/10 | Cost: 0.54 | Train/Valid Acc.: 11.26%/11.27% \n",
            "9/10 | Cost: 0.52 | Train/Valid Acc.: 11.26%/11.27% \n",
            "10/10 | Cost: 0.51 | Train/Valid Acc.: 11.26%/11.27% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 9.22 | Train/Valid Acc.: 51.76%/51.35% \n",
            "2/10 | Cost: 4.83 | Train/Valid Acc.: 63.67%/63.66% \n",
            "3/10 | Cost: 2.58 | Train/Valid Acc.: 60.95%/60.86% \n",
            "4/10 | Cost: 1.59 | Train/Valid Acc.: 63.87%/64.00% \n",
            "5/10 | Cost: 1.24 | Train/Valid Acc.: 64.94%/65.16% \n",
            "6/10 | Cost: 1.13 | Train/Valid Acc.: 65.31%/65.39% \n",
            "7/10 | Cost: 1.11 | Train/Valid Acc.: 67.53%/67.38% \n",
            "8/10 | Cost: 1.12 | Train/Valid Acc.: 69.30%/69.41% \n",
            "9/10 | Cost: 1.13 | Train/Valid Acc.: 70.56%/70.55% \n",
            "10/10 | Cost: 1.14 | Train/Valid Acc.: 70.44%/70.31% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 9.19 | Train/Valid Acc.: 54.88%/54.42% \n",
            "2/10 | Cost: 4.85 | Train/Valid Acc.: 58.73%/58.21% \n",
            "3/10 | Cost: 2.60 | Train/Valid Acc.: 65.28%/64.77% \n",
            "4/10 | Cost: 1.61 | Train/Valid Acc.: 62.80%/62.04% \n",
            "5/10 | Cost: 1.25 | Train/Valid Acc.: 62.37%/61.96% \n",
            "6/10 | Cost: 1.14 | Train/Valid Acc.: 63.10%/62.49% \n",
            "7/10 | Cost: 1.11 | Train/Valid Acc.: 66.48%/66.13% \n",
            "8/10 | Cost: 1.11 | Train/Valid Acc.: 67.15%/66.71% \n",
            "9/10 | Cost: 1.13 | Train/Valid Acc.: 68.89%/68.37% \n",
            "10/10 | Cost: 1.14 | Train/Valid Acc.: 71.92%/71.22% \n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "1/10 | Cost: 9.23 | Train/Valid Acc.: 62.61%/62.47% \n",
            "2/10 | Cost: 4.86 | Train/Valid Acc.: 61.99%/61.66% \n",
            "3/10 | Cost: 2.60 | Train/Valid Acc.: 63.04%/62.71% \n",
            "4/10 | Cost: 1.61 | Train/Valid Acc.: 63.40%/62.93% \n",
            "5/10 | Cost: 1.24 | Train/Valid Acc.: 63.85%/63.11% \n",
            "6/10 | Cost: 1.12 | Train/Valid Acc.: 66.08%/65.67% \n",
            "7/10 | Cost: 1.09 | Train/Valid Acc.: 68.00%/67.18% \n",
            "8/10 | Cost: 1.09 | Train/Valid Acc.: 69.36%/68.63% \n",
            "9/10 | Cost: 1.10 | Train/Valid Acc.: 68.23%/67.56% \n",
            "10/10 | Cost: 1.11 | Train/Valid Acc.: 70.33%/69.26% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 0.97 | Train/Valid Acc.: 62.35%/62.18% \n",
            "2/10 | Cost: 1.04 | Train/Valid Acc.: 76.79%/75.79% \n",
            "3/10 | Cost: 1.13 | Train/Valid Acc.: 82.50%/82.03% \n",
            "4/10 | Cost: 1.19 | Train/Valid Acc.: 84.27%/83.62% \n",
            "5/10 | Cost: 1.23 | Train/Valid Acc.: 86.71%/86.03% \n",
            "6/10 | Cost: 1.26 | Train/Valid Acc.: 87.51%/87.01% \n",
            "7/10 | Cost: 1.28 | Train/Valid Acc.: 88.28%/87.59% \n",
            "8/10 | Cost: 1.29 | Train/Valid Acc.: 88.33%/87.61% \n",
            "9/10 | Cost: 1.30 | Train/Valid Acc.: 88.55%/87.85% \n",
            "10/10 | Cost: 1.31 | Train/Valid Acc.: 88.97%/88.13% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 0.98 | Train/Valid Acc.: 61.18%/60.72% \n",
            "2/10 | Cost: 1.05 | Train/Valid Acc.: 76.05%/76.34% \n",
            "3/10 | Cost: 1.14 | Train/Valid Acc.: 81.63%/81.67% \n",
            "4/10 | Cost: 1.19 | Train/Valid Acc.: 84.53%/84.03% \n",
            "5/10 | Cost: 1.23 | Train/Valid Acc.: 86.68%/86.41% \n",
            "6/10 | Cost: 1.25 | Train/Valid Acc.: 87.63%/87.38% \n",
            "7/10 | Cost: 1.27 | Train/Valid Acc.: 87.96%/87.54% \n",
            "8/10 | Cost: 1.28 | Train/Valid Acc.: 87.96%/87.82% \n",
            "9/10 | Cost: 1.29 | Train/Valid Acc.: 88.07%/87.93% \n",
            "10/10 | Cost: 1.30 | Train/Valid Acc.: 88.42%/88.22% \n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "1/10 | Cost: 0.98 | Train/Valid Acc.: 59.61%/59.62% \n",
            "2/10 | Cost: 1.05 | Train/Valid Acc.: 75.38%/74.91% \n",
            "3/10 | Cost: 1.13 | Train/Valid Acc.: 81.95%/81.14% \n",
            "4/10 | Cost: 1.19 | Train/Valid Acc.: 84.98%/84.17% \n",
            "5/10 | Cost: 1.23 | Train/Valid Acc.: 86.83%/85.97% \n",
            "6/10 | Cost: 1.25 | Train/Valid Acc.: 87.67%/86.93% \n",
            "7/10 | Cost: 1.27 | Train/Valid Acc.: 88.11%/87.44% \n",
            "8/10 | Cost: 1.28 | Train/Valid Acc.: 88.24%/87.39% \n",
            "9/10 | Cost: 1.29 | Train/Valid Acc.: 88.94%/88.05% \n",
            "10/10 | Cost: 1.30 | Train/Valid Acc.: 88.98%/88.05% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 0.63 | Train/Valid Acc.: 18.24%/18.06% \n",
            "2/10 | Cost: 0.55 | Train/Valid Acc.: 11.19%/11.20% \n",
            "3/10 | Cost: 0.51 | Train/Valid Acc.: 11.19%/11.20% \n",
            "4/10 | Cost: 0.47 | Train/Valid Acc.: 11.19%/11.20% \n",
            "5/10 | Cost: 0.43 | Train/Valid Acc.: 10.50%/10.50% \n",
            "6/10 | Cost: 0.40 | Train/Valid Acc.: 11.19%/11.20% \n",
            "7/10 | Cost: 0.37 | Train/Valid Acc.: 11.19%/11.20% \n",
            "8/10 | Cost: 0.34 | Train/Valid Acc.: 10.50%/10.50% \n",
            "9/10 | Cost: 0.33 | Train/Valid Acc.: 11.19%/11.20% \n",
            "10/10 | Cost: 0.33 | Train/Valid Acc.: 11.19%/11.20% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 0.62 | Train/Valid Acc.: 10.32%/10.32% \n",
            "2/10 | Cost: 0.55 | Train/Valid Acc.: 11.30%/11.30% \n",
            "3/10 | Cost: 0.51 | Train/Valid Acc.: 10.31%/10.31% \n",
            "4/10 | Cost: 0.47 | Train/Valid Acc.: 11.30%/11.30% \n",
            "5/10 | Cost: 0.43 | Train/Valid Acc.: 11.30%/11.30% \n",
            "6/10 | Cost: 0.40 | Train/Valid Acc.: 11.30%/11.30% \n",
            "7/10 | Cost: 0.37 | Train/Valid Acc.: 10.31%/10.31% \n",
            "8/10 | Cost: 0.34 | Train/Valid Acc.: 9.76%/9.76% \n",
            "9/10 | Cost: 0.33 | Train/Valid Acc.: 10.31%/10.31% \n",
            "10/10 | Cost: 0.33 | Train/Valid Acc.: 11.30%/11.30% \n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "1/10 | Cost: 0.63 | Train/Valid Acc.: 11.26%/11.27% \n",
            "2/10 | Cost: 0.55 | Train/Valid Acc.: 10.44%/10.44% \n",
            "3/10 | Cost: 0.51 | Train/Valid Acc.: 11.26%/11.27% \n",
            "4/10 | Cost: 0.47 | Train/Valid Acc.: 11.26%/11.27% \n",
            "5/10 | Cost: 0.43 | Train/Valid Acc.: 11.26%/11.27% \n",
            "6/10 | Cost: 0.40 | Train/Valid Acc.: 10.12%/10.12% \n",
            "7/10 | Cost: 0.37 | Train/Valid Acc.: 10.44%/10.44% \n",
            "8/10 | Cost: 0.35 | Train/Valid Acc.: 11.26%/11.27% \n",
            "9/10 | Cost: 0.33 | Train/Valid Acc.: 11.26%/11.27% \n",
            "10/10 | Cost: 0.33 | Train/Valid Acc.: 11.26%/11.27% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 1.14 | Train/Valid Acc.: 53.76%/53.74% \n",
            "2/10 | Cost: 1.09 | Train/Valid Acc.: 69.37%/69.75% \n",
            "3/10 | Cost: 1.11 | Train/Valid Acc.: 77.94%/77.96% \n",
            "4/10 | Cost: 1.14 | Train/Valid Acc.: 80.97%/80.88% \n",
            "5/10 | Cost: 1.16 | Train/Valid Acc.: 83.50%/83.20% \n",
            "6/10 | Cost: 1.19 | Train/Valid Acc.: 85.45%/85.31% \n",
            "7/10 | Cost: 1.20 | Train/Valid Acc.: 85.93%/85.63% \n",
            "8/10 | Cost: 1.22 | Train/Valid Acc.: 86.71%/86.51% \n",
            "9/10 | Cost: 1.23 | Train/Valid Acc.: 87.51%/87.29% \n",
            "10/10 | Cost: 1.24 | Train/Valid Acc.: 87.70%/87.57% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 1.12 | Train/Valid Acc.: 54.28%/54.38% \n",
            "2/10 | Cost: 1.07 | Train/Valid Acc.: 69.74%/70.23% \n",
            "3/10 | Cost: 1.09 | Train/Valid Acc.: 77.14%/77.34% \n",
            "4/10 | Cost: 1.12 | Train/Valid Acc.: 81.31%/81.38% \n",
            "5/10 | Cost: 1.15 | Train/Valid Acc.: 83.47%/83.68% \n",
            "6/10 | Cost: 1.18 | Train/Valid Acc.: 85.39%/85.46% \n",
            "7/10 | Cost: 1.19 | Train/Valid Acc.: 86.48%/86.41% \n",
            "8/10 | Cost: 1.21 | Train/Valid Acc.: 87.33%/87.18% \n",
            "9/10 | Cost: 1.22 | Train/Valid Acc.: 87.68%/87.40% \n",
            "10/10 | Cost: 1.23 | Train/Valid Acc.: 88.00%/87.82% \n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "1/10 | Cost: 1.12 | Train/Valid Acc.: 45.91%/45.61% \n",
            "2/10 | Cost: 1.07 | Train/Valid Acc.: 68.40%/68.60% \n",
            "3/10 | Cost: 1.10 | Train/Valid Acc.: 77.84%/77.24% \n",
            "4/10 | Cost: 1.13 | Train/Valid Acc.: 80.92%/80.47% \n",
            "5/10 | Cost: 1.17 | Train/Valid Acc.: 83.09%/82.77% \n",
            "6/10 | Cost: 1.19 | Train/Valid Acc.: 84.95%/84.61% \n",
            "7/10 | Cost: 1.21 | Train/Valid Acc.: 86.11%/85.64% \n",
            "8/10 | Cost: 1.23 | Train/Valid Acc.: 86.75%/86.17% \n",
            "9/10 | Cost: 1.24 | Train/Valid Acc.: 87.81%/87.31% \n",
            "10/10 | Cost: 1.25 | Train/Valid Acc.: 88.22%/87.56% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 1.33 | Train/Valid Acc.: 56.06%/55.92% \n",
            "2/10 | Cost: 1.24 | Train/Valid Acc.: 68.46%/68.22% \n",
            "3/10 | Cost: 1.24 | Train/Valid Acc.: 76.50%/76.48% \n",
            "4/10 | Cost: 1.26 | Train/Valid Acc.: 80.78%/81.12% \n",
            "5/10 | Cost: 1.28 | Train/Valid Acc.: 84.23%/84.36% \n",
            "6/10 | Cost: 1.31 | Train/Valid Acc.: 85.99%/86.33% \n",
            "7/10 | Cost: 1.33 | Train/Valid Acc.: 87.00%/87.19% \n",
            "8/10 | Cost: 1.35 | Train/Valid Acc.: 87.36%/87.67% \n",
            "9/10 | Cost: 1.36 | Train/Valid Acc.: 87.42%/87.76% \n",
            "10/10 | Cost: 1.37 | Train/Valid Acc.: 88.31%/88.54% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 1.31 | Train/Valid Acc.: 47.94%/48.36% \n",
            "2/10 | Cost: 1.23 | Train/Valid Acc.: 74.12%/74.00% \n",
            "3/10 | Cost: 1.25 | Train/Valid Acc.: 81.11%/81.17% \n",
            "4/10 | Cost: 1.29 | Train/Valid Acc.: 84.18%/84.31% \n",
            "5/10 | Cost: 1.32 | Train/Valid Acc.: 84.97%/84.75% \n",
            "6/10 | Cost: 1.34 | Train/Valid Acc.: 86.62%/86.38% \n",
            "7/10 | Cost: 1.35 | Train/Valid Acc.: 87.84%/87.54% \n",
            "8/10 | Cost: 1.36 | Train/Valid Acc.: 87.87%/87.56% \n",
            "9/10 | Cost: 1.37 | Train/Valid Acc.: 88.47%/88.21% \n",
            "10/10 | Cost: 1.38 | Train/Valid Acc.: 88.52%/88.30% \n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "1/10 | Cost: 1.31 | Train/Valid Acc.: 57.45%/56.92% \n",
            "2/10 | Cost: 1.24 | Train/Valid Acc.: 72.62%/72.90% \n",
            "3/10 | Cost: 1.26 | Train/Valid Acc.: 78.44%/78.81% \n",
            "4/10 | Cost: 1.29 | Train/Valid Acc.: 81.64%/81.72% \n",
            "5/10 | Cost: 1.31 | Train/Valid Acc.: 84.43%/84.33% \n",
            "6/10 | Cost: 1.32 | Train/Valid Acc.: 85.68%/85.82% \n",
            "7/10 | Cost: 1.33 | Train/Valid Acc.: 86.58%/86.39% \n",
            "8/10 | Cost: 1.34 | Train/Valid Acc.: 87.51%/87.18% \n",
            "9/10 | Cost: 1.35 | Train/Valid Acc.: 88.02%/87.53% \n",
            "10/10 | Cost: 1.36 | Train/Valid Acc.: 88.15%/87.80% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 1.30 | Train/Valid Acc.: 11.19%/11.20% \n",
            "2/10 | Cost: 0.61 | Train/Valid Acc.: 11.19%/11.20% \n",
            "3/10 | Cost: 0.57 | Train/Valid Acc.: 11.19%/11.20% \n",
            "4/10 | Cost: 0.54 | Train/Valid Acc.: 10.17%/10.17% \n",
            "5/10 | Cost: 0.52 | Train/Valid Acc.: 11.19%/11.20% \n",
            "6/10 | Cost: 0.49 | Train/Valid Acc.: 10.50%/10.50% \n",
            "7/10 | Cost: 0.47 | Train/Valid Acc.: 11.19%/11.20% \n",
            "8/10 | Cost: 0.44 | Train/Valid Acc.: 9.73%/9.73% \n",
            "9/10 | Cost: 0.42 | Train/Valid Acc.: 10.11%/10.12% \n",
            "10/10 | Cost: 0.40 | Train/Valid Acc.: 11.19%/11.20% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "1/10 | Cost: 1.33 | Train/Valid Acc.: 11.30%/11.30% \n",
            "2/10 | Cost: 0.61 | Train/Valid Acc.: 11.30%/11.30% \n",
            "3/10 | Cost: 0.57 | Train/Valid Acc.: 10.31%/10.31% \n",
            "4/10 | Cost: 0.54 | Train/Valid Acc.: 11.30%/11.30% \n",
            "5/10 | Cost: 0.52 | Train/Valid Acc.: 11.30%/11.30% \n",
            "6/10 | Cost: 0.49 | Train/Valid Acc.: 11.30%/11.30% \n",
            "7/10 | Cost: 0.47 | Train/Valid Acc.: 9.94%/9.94% \n",
            "8/10 | Cost: 0.44 | Train/Valid Acc.: 11.30%/11.30% \n",
            "9/10 | Cost: 0.42 | Train/Valid Acc.: 11.30%/11.30% \n",
            "10/10 | Cost: 0.40 | Train/Valid Acc.: 10.31%/10.31% \n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n",
            "1/10 | Cost: 1.32 | Train/Valid Acc.: 23.35%/23.44% \n",
            "2/10 | Cost: 0.61 | Train/Valid Acc.: 11.26%/11.27% \n",
            "3/10 | Cost: 0.57 | Train/Valid Acc.: 11.26%/11.27% \n",
            "4/10 | Cost: 0.55 | Train/Valid Acc.: 11.26%/11.27% \n",
            "5/10 | Cost: 0.52 | Train/Valid Acc.: 11.26%/11.27% \n",
            "6/10 | Cost: 0.49 | Train/Valid Acc.: 11.26%/11.27% \n",
            "7/10 | Cost: 0.47 | Train/Valid Acc.: 11.26%/11.27% \n",
            "8/10 | Cost: 0.44 | Train/Valid Acc.: 9.98%/9.97% \n",
            "9/10 | Cost: 0.42 | Train/Valid Acc.: 11.26%/11.27% \n",
            "10/10 | Cost: 0.40 | Train/Valid Acc.: 11.26%/11.27% \n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "20488272\n",
            "8780800\n",
            "26133\n",
            "11200\n",
            "20488272\n",
            "8781584\n",
            "26133\n",
            "11201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aryad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
            "9 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\aryad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\aryad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\aryad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"C:\\Users\\aryad\\AppData\\Local\\Temp\\ipykernel_5092\\4064795737.py\", line 407, in fit\n",
            "    self.compile(X_train.shape[1], np.unique(y_train).shape[0])\n",
            "  File \"C:\\Users\\aryad\\AppData\\Local\\Temp\\ipykernel_5092\\4064795737.py\", line 89, in compile\n",
            "    raise ValueError(\"Invalid initializing technique\")\n",
            "ValueError: Invalid initializing technique\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\aryad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.11253571        nan 0.70787487 0.88426785 0.11253571        nan\n",
            " 0.87473214 0.88035707 0.10864292        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30732800\n",
            "13171200\n",
            "39200\n",
            "16800\n",
            "1/10 | Cost: 1.00 | Train/Valid Acc.: 67.68%/67.52% \n",
            "2/10 | Cost: 1.13 | Train/Valid Acc.: 82.18%/81.74% \n",
            "3/10 | Cost: 1.21 | Train/Valid Acc.: 85.92%/85.68% \n",
            "4/10 | Cost: 1.25 | Train/Valid Acc.: 87.05%/86.96% \n",
            "5/10 | Cost: 1.28 | Train/Valid Acc.: 88.06%/88.04% \n",
            "6/10 | Cost: 1.29 | Train/Valid Acc.: 88.08%/87.92% \n",
            "7/10 | Cost: 1.31 | Train/Valid Acc.: 88.49%/88.46% \n",
            "8/10 | Cost: 1.31 | Train/Valid Acc.: 88.78%/88.67% \n",
            "9/10 | Cost: 1.32 | Train/Valid Acc.: 89.03%/88.94% \n",
            "10/10 | Cost: 1.32 | Train/Valid Acc.: 88.85%/88.70% \n"
          ]
        }
      ],
      "source": [
        "parameter_grid = {'classifier__l1': np.arange(0, 0.011, 0.002),\n",
        "                 'classifier__l2': np.arange(0, 0.011, 0.002),\n",
        "                 'classifier__n_hidden': np.arange(20, 101, 10),\n",
        "                 'classifier__eta': np.arange(0.0001, 0.0011, 0.0001),\n",
        "                 'classifier__init_technique': ['normal', 'xavier', 'he'] \n",
        "}\n",
        "random_search = RandomizedSearchCV(pipe, param_distributions=parameter_grid, cv=3, n_iter=10, random_state=RANDOM_STATE)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_classifier = random_search.best_estimator_ \n",
        "best_score = random_search.best_score_ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "Zisqj2Qk6318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline(steps=[('normalizer', Normalizer()),\n",
            "                ('classifier',\n",
            "                 FullyConnectedNetwork(epochs=10, eta=0.0008,\n",
            "                                       init_technique='he', l2=0.008,\n",
            "                                       n_hidden=50))]) 0.8842678545179812\n"
          ]
        }
      ],
      "source": [
        "# PRINT THE SCORES HERE\n",
        "print(best_classifier, best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9gSkzvszU1K"
      },
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
